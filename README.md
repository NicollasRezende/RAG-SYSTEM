# ü§ñ Sistema RAG - Retrieval-Augmented Generation

> Sistema completo de IA com recupera√ß√£o aumentada de documentos usando Python, Ollama e ChromaDB

[![Python](https://img.shields.io/badge/Python-3.12+-blue.svg)](https://www.python.org/)
[![LangChain](https://img.shields.io/badge/LangChain-0.3+-green.svg)](https://python.langchain.com/)
[![Ollama](https://img.shields.io/badge/Ollama-Local%20LLM-orange.svg)](https://ollama.ai/)

## üìã √çndice

- [Sobre o Projeto](#-sobre-o-projeto)
- [Como Funciona](#-como-funciona)
- [Caracter√≠sticas](#-caracter√≠sticas)
- [Tecnologias](#-tecnologias)
- [Instala√ß√£o](#-instala√ß√£o)
- [Uso](#-uso)
- [Arquitetura](#-arquitetura)
- [Exemplos](#-exemplos)
- [Configura√ß√£o Avan√ßada](#-configura√ß√£o-avan√ßada)
- [FAQ](#-faq)
- [Contribuindo](#-contribuindo)
- [Licen√ßa](#-licen√ßa)

## üéØ Sobre o Projeto

Este √© um sistema RAG (Retrieval-Augmented Generation) completo que permite fazer perguntas sobre seus documentos usando IA. Diferente de chatbots tradicionais, o RAG busca informa√ß√µes nos seus documentos antes de gerar uma resposta, garantindo respostas precisas e baseadas em fontes confi√°veis.

**üéÅ 100% Gratuito** - N√£o usa APIs pagas, roda completamente local usando Ollama!

### Problema que resolve:
- ‚ùå LLMs tradicionais "alucinam" e inventam informa√ß√µes
- ‚ùå Respostas gen√©ricas sem base em seus dados espec√≠ficos
- ‚ùå Custos elevados de APIs (OpenAI, Anthropic, etc)
- ‚ùå Privacidade: seus dados v√£o para servidores externos

### Solu√ß√£o:
- ‚úÖ Respostas baseadas apenas nos seus documentos
- ‚úÖ Transpar√™ncia: mostra as fontes utilizadas
- ‚úÖ 100% gratuito: roda localmente com Ollama
- ‚úÖ Privacidade total: seus dados n√£o saem do seu computador

## üîç Como Funciona

```mermaid
graph LR
    A[Seus Documentos] -->|1. Carregar| B[Document Loader]
    B -->|2. Dividir em chunks| C[Text Splitter]
    C -->|3. Criar embeddings| D[Sentence Transformers]
    D -->|4. Armazenar| E[ChromaDB]

    F[Sua Pergunta] -->|5. Buscar| E
    E -->|6. Docs Relevantes| G[Retriever]
    G -->|7. Contexto| H[Ollama LLM]
    H -->|8. Resposta| I[Usu√°rio]
```

### Fluxo Detalhado:

1. **Indexa√ß√£o** (executado uma vez):
   ```
   Documentos ‚Üí Chunks ‚Üí Embeddings ‚Üí Vector Store
   ```
   - Documentos s√£o divididos em peda√ßos menores (chunks)
   - Cada chunk √© convertido em um vetor num√©rico (embedding)
   - Vetores s√£o armazenados no ChromaDB para busca r√°pida

2. **Consulta** (cada pergunta):
   ```
   Pergunta ‚Üí Embedding ‚Üí Busca Sem√¢ntica ‚Üí Documentos Relevantes ‚Üí LLM ‚Üí Resposta
   ```
   - Sua pergunta √© convertida em embedding
   - Busca os documentos mais similares no ChromaDB
   - Envia documentos + pergunta para o LLM
   - LLM gera resposta baseada apenas nos documentos

## ‚ú® Caracter√≠sticas

### Funcionalidades Principais

- üìÑ **M√∫ltiplos Formatos**: PDF, DOCX, TXT, Markdown
- üîç **Busca Sem√¢ntica**: Encontra documentos por significado, n√£o apenas palavras-chave
- ü§ñ **IA Local**: Usa Ollama - sem custos, sem internet ap√≥s setup
- üíæ **Persist√™ncia**: Vector store salvo em disco - n√£o precisa reprocessar
- üîó **Rastreamento de Fontes**: Mostra quais documentos foram usados
- üé® **Interface Interativa**: Terminal amig√°vel com perguntas e respostas
- ‚ö° **Performance**: ChromaDB otimizado para buscas r√°pidas

### Diferenciais T√©cnicos

- üèóÔ∏è **Arquitetura Modular**: F√°cil de estender e customizar
- üìä **Embeddings de Alta Qualidade**: Sentence Transformers otimizados
- üîß **Configur√°vel**: Ajuste chunk size, overlap, n√∫mero de documentos, etc
- üê≥ **Pronto para Produ√ß√£o**: C√≥digo limpo, documentado e test√°vel

## üõ†Ô∏è Tecnologias

### Core Stack

| Tecnologia | Vers√£o | Fun√ß√£o |
|------------|--------|--------|
| **Python** | 3.12+ | Linguagem principal |
| **LangChain** | 0.3+ | Framework de orquestra√ß√£o |
| **Ollama** | Latest | LLM local gratuito |
| **ChromaDB** | 0.5+ | Banco de dados vetorial |
| **Sentence Transformers** | 2.3+ | Gera√ß√£o de embeddings |

### Bibliotecas Auxiliares

- **PyPDF**: Leitura de arquivos PDF
- **python-docx**: Leitura de documentos Word
- **NumPy/SciPy**: Opera√ß√µes num√©ricas
- **python-dotenv**: Gerenciamento de vari√°veis de ambiente

## üì• Instala√ß√£o

### Pr√©-requisitos

- Python 3.12 ou superior
- 8GB RAM (m√≠nimo) | 16GB+ (recomendado)
- 10GB de espa√ßo em disco

### Passo 1: Clonar o Reposit√≥rio

```bash
git clone https://github.com/seu-usuario/rag-system.git
cd rag-system
```

### Passo 2: Criar Ambiente Virtual

```bash
python -m venv venv

# Linux/Mac
source venv/bin/activate

# Windows
venv\Scripts\activate
```

### Passo 3: Instalar Depend√™ncias

```bash
pip install -r requirements.txt
```

### Passo 4: Instalar Ollama

#### Linux
```bash
curl -fsSL https://ollama.ai/install.sh | sh
```

#### Windows
Baixe o instalador: [ollama.ai/download/windows](https://ollama.ai/download/windows)

#### macOS
```bash
brew install ollama
```

### Passo 5: Baixar um Modelo

```bash
# Modelo pequeno e r√°pido (recomendado para come√ßar)
ollama pull qwen2.5:1.5b

# Alternativas:
ollama pull llama3.2      # ~2GB - excelente qualidade
ollama pull mistral       # ~4GB - muito r√°pido
ollama pull qwen2.5:7b    # ~5GB - melhor qualidade
```

## üöÄ Uso

### Modo B√°sico

1. **Adicione seus documentos** na pasta `data/`:
```bash
cp seus_documentos.pdf data/
cp sua_base_conhecimento.txt data/
```

2. **Execute o sistema**:
```bash
python main.py
```

3. **Fa√ßa perguntas**:
```
üí¨ Pergunta: O que √© RAG?
ü§ñ Resposta: RAG (Retrieval-Augmented Generation) √© uma t√©cnica...
üìö Fontes utilizadas: data/exemplo.txt
```

### Modo Program√°tico

```python
from src.document_loader import DocumentLoader
from src.vector_store import VectorStore
from src.rag_pipeline import RAGPipeline

# 1. Carrega documentos
loader = DocumentLoader(chunk_size=1000, chunk_overlap=200)
documents = loader.load_directory("./data")

# 2. Cria vector store
vector_store = VectorStore(
    persist_directory="./chroma_db",
    embedding_model="sentence-transformers/all-MiniLM-L6-v2"
)
vector_store.create_vectorstore(documents)

# 3. Configura RAG
rag = RAGPipeline(
    vector_store=vector_store,
    model_name="qwen2.5:1.5b"
)
rag.setup_qa_chain(search_kwargs={'k': 3})

# 4. Faz pergunta
result = rag.query("Qual √© o conte√∫do principal?")
print(result['answer'])
```

## üèóÔ∏è Arquitetura

### Estrutura de Diret√≥rios

```
rag_system/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ document_loader.py      # Carregamento de documentos
‚îÇ   ‚îú‚îÄ‚îÄ vector_store.py          # Gerenciamento do ChromaDB
‚îÇ   ‚îî‚îÄ‚îÄ rag_pipeline.py          # Pipeline RAG principal
‚îú‚îÄ‚îÄ data/                        # Seus documentos
‚îú‚îÄ‚îÄ chroma_db/                   # Vector store (gerado)
‚îú‚îÄ‚îÄ main.py                      # Script principal
‚îú‚îÄ‚îÄ requirements.txt             # Depend√™ncias Python
‚îú‚îÄ‚îÄ INSTALL_OLLAMA.md           # Guia de instala√ß√£o Ollama
‚îî‚îÄ‚îÄ README.md                    # Este arquivo
```

### Componentes Principais

#### 1. Document Loader (`src/document_loader.py`)

Respons√°vel por:
- Carregar documentos de diferentes formatos
- Dividir em chunks otimizados
- Preservar metadados

```python
class DocumentLoader:
    def load_single_file(file_path) -> List[Document]
    def load_directory(directory_path) -> List[Document]
    def load_text(text, metadata) -> List[Document]
```

#### 2. Vector Store (`src/vector_store.py`)

Respons√°vel por:
- Gerar embeddings dos documentos
- Armazenar no ChromaDB
- Realizar buscas de similaridade

```python
class VectorStore:
    def create_vectorstore(documents) -> Chroma
    def load_vectorstore() -> Chroma
    def similarity_search(query, k) -> List[Document]
    def get_retriever() -> Retriever
```

#### 3. RAG Pipeline (`src/rag_pipeline.py`)

Respons√°vel por:
- Integrar retriever + LLM
- Gerenciar prompts
- Gerar respostas contextualizadas

```python
class RAGPipeline:
    def setup_qa_chain(prompt_template, search_kwargs)
    def query(question) -> Dict[str, Any]
    def simple_retrieval(query) -> List[Document]
```

## üìö Exemplos

### Exemplo 1: Documenta√ß√£o T√©cnica

**Caso de uso**: Empresa com documenta√ß√£o interna extensa

```bash
# Adicionar documenta√ß√£o
cp docs/*.md data/
cp manuais/*.pdf data/

# Perguntas poss√≠veis:
"Como configurar o ambiente de desenvolvimento?"
"Quais s√£o as pol√≠ticas de seguran√ßa?"
"Onde est√° documentada a API de autentica√ß√£o?"
```

### Exemplo 2: Base de Conhecimento Acad√™mica

**Caso de uso**: Estudante pesquisando papers

```bash
# Adicionar papers
cp research_papers/*.pdf data/

# Perguntas poss√≠veis:
"Quais s√£o as principais t√©cnicas de RAG?"
"Compare os resultados dos experimentos"
"Quais datasets foram utilizados?"
```

### Exemplo 3: Atendimento ao Cliente

**Caso de uso**: FAQ e pol√≠ticas da empresa

```bash
# Adicionar FAQs e pol√≠ticas
cp faq.txt data/
cp politicas/*.docx data/

# Perguntas poss√≠veis:
"Qual √© a pol√≠tica de reembolso?"
"Como rastrear meu pedido?"
"Quais s√£o os prazos de entrega?"
```

## ‚öôÔ∏è Configura√ß√£o Avan√ßada

### Ajustar Tamanho dos Chunks

```python
loader = DocumentLoader(
    chunk_size=1000,      # Tamanho do chunk (caracteres)
    chunk_overlap=200     # Sobreposi√ß√£o entre chunks
)
```

**Quando ajustar:**
- Chunks pequenos (500): documentos t√©cnicos, perguntas espec√≠ficas
- Chunks m√©dios (1000): uso geral
- Chunks grandes (2000): narrativas, contexto amplo

### Escolher Modelo de Embeddings

```python
vector_store = VectorStore(
    embedding_model="sentence-transformers/all-MiniLM-L6-v2"  # Padr√£o
    # embedding_model="sentence-transformers/all-mpnet-base-v2"  # Mais preciso
    # embedding_model="paraphrase-multilingual-MiniLM-L12-v2"  # Multil√≠ngue
)
```

### Configurar N√∫mero de Documentos Recuperados

```python
rag.setup_qa_chain(
    search_kwargs={'k': 3}  # Retorna top 3 documentos mais relevantes
)
```

**Recomenda√ß√µes:**
- k=2-3: Respostas focadas e r√°pidas
- k=4-5: Mais contexto, respostas completas
- k=6+: Perguntas complexas que exigem m√∫ltiplas fontes

### Customizar Prompts

```python
custom_prompt = """Voc√™ √© um assistente especializado em tecnologia.

Contexto: {context}

Pergunta: {question}

Instru√ß√µes:
- Use termos t√©cnicos precisos
- Cite exemplos do contexto
- Seja conciso mas completo

Resposta:"""

rag.setup_qa_chain(prompt_template=custom_prompt)
```

### Filtrar por Metadados

```python
# Buscar apenas em documentos espec√≠ficos
results = vector_store.similarity_search(
    "sua consulta",
    k=4,
    filter_metadata={"source": "manual_tecnico.pdf"}
)
```

## üé® Modelos Ollama Dispon√≠veis

### Compara√ß√£o de Modelos

| Modelo | Tamanho | RAM | Qualidade | Velocidade | Melhor Para |
|--------|---------|-----|-----------|------------|-------------|
| **qwen2.5:1.5b** | 1GB | 4GB | ‚≠ê‚≠ê‚≠ê | ‚ö°‚ö°‚ö°‚ö°‚ö° | PCs fracos, testes |
| **llama3.2** | 2GB | 8GB | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚ö°‚ö°‚ö°‚ö° | Uso geral |
| **mistral** | 4GB | 8GB | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚ö°‚ö°‚ö°‚ö°‚ö° | Velocidade m√°xima |
| **qwen2.5:7b** | 5GB | 16GB | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚ö°‚ö°‚ö° | Melhor qualidade |
| **gemma2** | 5GB | 16GB | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚ö°‚ö°‚ö°‚ö° | Google, balanceado |

### Trocar de Modelo

```bash
# Baixar novo modelo
ollama pull llama3.2

# No pr√≥ximo uso, escolha o novo modelo quando perguntado
python main.py
# > Escolha o modelo: llama3.2
```

## ‚ùì FAQ

### Por que usar RAG ao inv√©s de apenas um LLM?

**LLM tradicional:**
- Conhecimento limitado ao treinamento
- Pode "alucinar" informa√ß√µes
- N√£o sabe sobre seus dados espec√≠ficos

**RAG:**
- Acesso aos seus documentos espec√≠ficos
- Respostas baseadas em fontes verific√°veis
- Atualiza√ß√£o simples (adicione novos docs)

### √â realmente gratuito?

Sim! Usa apenas ferramentas open source:
- Ollama: gratuito
- ChromaDB: gratuito
- Sentence Transformers: gratuito
- LangChain: gratuito

**Custo total: R$ 0,00**

### Funciona offline?

Sim! Ap√≥s baixar o modelo do Ollama e instalar as depend√™ncias, funciona 100% offline.

### Quanto de RAM preciso?

- **M√≠nimo**: 8GB (modelos pequenos)
- **Recomendado**: 16GB (modelos m√©dios)
- **Ideal**: 32GB+ (modelos grandes)

### Posso usar GPU?

Sim! Ollama detecta e usa GPU automaticamente:
- NVIDIA (CUDA)
- AMD (ROCm)
- Apple Silicon (Metal)

### Como adicionar mais documentos?

```bash
# Copie os documentos para data/
cp novos_documentos/* data/

# Execute novamente
python main.py
# Escolha "s" quando perguntar sobre adicionar novos documentos
```

### Os documentos ficam salvos?

Sim! O ChromaDB persiste os embeddings em disco na pasta `chroma_db/`. Voc√™ s√≥ precisa reprocessar se adicionar novos documentos.

### Posso usar APIs externas (OpenAI, Anthropic)?

Sim! O c√≥digo √© modular. Basta modificar `rag_pipeline.py` para usar outro LLM.

### Suporta outros idiomas?

Sim! Use um modelo de embeddings multil√≠ngue:
```python
embedding_model="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
```

## üêõ Troubleshooting

### Erro: "Ollama call failed with status code 404"

**Causa**: Modelo n√£o encontrado

**Solu√ß√£o**:
```bash
# Liste modelos instalados
ollama list

# Baixe o modelo necess√°rio
ollama pull qwen2.5:1.5b
```

### Erro: "connection refused"

**Causa**: Ollama n√£o est√° rodando

**Solu√ß√£o**:
```bash
# Inicie o Ollama
ollama serve
```

### Sistema muito lento

**Causas e solu√ß√µes**:

1. **Modelo muito grande**:
   ```bash
   ollama pull qwen2.5:1.5b  # Use modelo menor
   ```

2. **Muitos documentos**:
   - Reduza o n√∫mero de documentos
   - Aumente a RAM

3. **Chunks muito grandes**:
   ```python
   DocumentLoader(chunk_size=500)  # Reduza chunk size
   ```

### Respostas irrelevantes

**Solu√ß√µes**:

1. **Aumente documentos recuperados**:
   ```python
   rag.setup_qa_chain(search_kwargs={'k': 5})
   ```

2. **Ajuste chunk size**:
   ```python
   DocumentLoader(chunk_size=1500, chunk_overlap=300)
   ```

3. **Use modelo melhor**:
   ```bash
   ollama pull llama3.2
   ```


## üìû Contato

**Seu Nome**
- GitHub: [@NicollasRezende](github.com/NicollasRezende)
- LinkedIn: [Nicollas Rezende](linkedin.com/in/nicollas-rezende)
- Email: nicollaspereirarezende@outlook.com.br

